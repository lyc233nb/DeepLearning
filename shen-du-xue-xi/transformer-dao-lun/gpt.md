# ğŸ˜€ GPT

GPTå…¨ç§°ä¸ºGenerative Pre-trained Transformerï¼Œå®ƒä½¿ç”¨äº†Transformerä¸­çš„Decoderæ¶æ„ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡çš„æ— ç›‘ç£é¢„è®­ç»ƒæ¥æé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒGPTé€šè¿‡å¤„ç†å¤§é‡çš„æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®ï¼Œå­¦ä¹ åˆ°äº†è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨å®é™…åº”ç”¨æ—¶ï¼ŒGPTå¯ä»¥é€šè¿‡å¾®è°ƒï¼ˆfine-tuningï¼‰çš„æ–¹å¼ï¼Œæ ¹æ®å…·ä½“ä»»åŠ¡çš„éœ€æ±‚ï¼Œå¯¹é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®å°çš„è°ƒæ•´ï¼Œä»è€Œé€‚åº”ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç­‰ä»»åŠ¡ã€‚GPTåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œå¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†å¾ˆå¥½çš„æˆç»©ã€‚



<figure><img src="../../.gitbook/assets/language_understanding_paper_2.jpg" alt=""><figcaption><p>GPTä¸­çš„Decoder</p></figcaption></figure>

## æ— ç›‘ç£çš„é¢„è®­ç»ƒ

GPTçš„æ— ç›‘ç£é¢„è®­ç»ƒå°±æ˜¯è®©æ¨¡å‹è‡ªå·±å­¦ä¹ è¯­è¨€çš„è§„å¾‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œæ— éœ€äººä¸ºæ ‡æ³¨æ•°æ®ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒGPTä½¿ç”¨äº†å¤§é‡çš„æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ç»´åŸºç™¾ç§‘ã€äº’è”ç½‘æ–°é—»ç­‰å¤§è§„æ¨¡è¯­æ–™åº“ã€‚GPTå°†è¿™äº›æ–‡æœ¬æ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œé€šè¿‡ä¸æ–­åœ°å­¦ä¹ è¯­è¨€ä¸­çš„ç»Ÿè®¡è§„å¾‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚

åœ¨è¿™ä¸ªé˜¶æ®µä¸­ï¼ŒGPTæœ€æ—©æœŸä½¿ç”¨çš„æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’æ¨¡å‹çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡æœ€å¤§åŒ–ç»™å®šè¾“å…¥åºåˆ—çš„ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚

è‡ªå›å½’æ¨¡å‹çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ¨¡å‹å¯¹æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®çš„ä¼¼ç„¶æ€§ï¼Œå³æœ€å¤§åŒ–æ¨¡å‹åœ¨ç»™å®šæ— æ ‡æ³¨æ–‡æœ¬æ•°æ®ä¸‹çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°ã€‚æˆ‘ä»¬å¸Œæœ›è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¯ä»¥åœ¨å½“å‰è¾“å…¥æ–‡æœ¬åºåˆ—çš„åŸºç¡€ä¸Šï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ã€‚è€Œé¢„æµ‹æ¦‚ç‡çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡å°±æ˜¯ä¼¼ç„¶æ€§ï¼Œå³å½“å‰æ¨¡å‹é¢„æµ‹çš„ç»“æœä¸å®é™…è§‚æµ‹å€¼ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ— æ ‡æ³¨æ–‡æœ¬æ•°æ®é›† $$D = {x_1, x_2, ..., x_N}$$ï¼Œå…¶ä¸­æ¯ä¸ª $$x_i$$æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º $$T_i$$çš„æ–‡æœ¬åºåˆ—ï¼Œè€Œæ¨¡å‹çš„å‚æ•°ä¸º $$\theta$$ã€‚å‡è®¾æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå°†$x\_i$ä¸­çš„æ¯ä¸ªå•è¯è¡¨ç¤ºä¸º $${w_{i,1}, w_{i,2}, ..., w_{i,T_i}}$$ï¼Œé‚£ä¹ˆæ¨¡å‹å¯¹äº $$x_i$$çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$\log p(x_i|\theta)=\sum_{t=1}^{T_i}\log p(w_{i,t}|w_{i,<t},\theta)$$

å…¶ä¸­ï¼Œ $$p(w_{i,t}|w_{i, <t},\theta)$$è¡¨ç¤ºç»™å®šä¸Šæ–‡ $$w_{i, <t}$$çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯¹äº$w\_{i,t}$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚

åœ¨GPT2ï¼ŒGPT3ä¸­åœ¨é¢„è®­ç»ƒé˜¶æ®µè¿˜å¼•å…¥äº†æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼ŒMasked Language Modelï¼Œå’ŒBertä¸­çš„ä¸€æ ·ï¼‰

MLMçš„ç›®æ ‡æ˜¯åœ¨è¾“å…¥åºåˆ—ä¸­éšæœºé®ç›–ä¸€äº›å•è¯ï¼Œå¹¶è®©æ¨¡å‹é¢„æµ‹è¿™äº›è¢«é®ç›–çš„å•è¯ã€‚

æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelï¼ŒMLMï¼‰çš„ä¼¼ç„¶å‡½æ•°è¡¨ç¤ºä¸ºï¼š

$$L_{MLM}=\prod_{i=1}^{N}P(w_{i}|w_{<i},w_{>i})$$

å…¶ä¸­ï¼Œ $$w_{i}$$è¡¨ç¤ºç¬¬ $$i$$ä¸ªä½ç½®çš„è¢«é®è”½çš„å•è¯ï¼Œé€šå¸¸åœ¨æ–‡æœ¬ä¸­ç”¨ä¸€ä¸ªç‰¹æ®Šç¬¦å·â€œ\[MASK]â€æ ‡è®°ï¼Œ $$w_{<i}$$è¡¨ç¤ºç¬¬$i$ä¸ªä½ç½®ä¹‹å‰çš„å•è¯åºåˆ—ï¼Œ $$w_{>i}$$è¡¨ç¤ºç¬¬$i$ä¸ªä½ç½®ä¹‹åçš„å•è¯åºåˆ—ï¼Œ $$N$$è¡¨ç¤ºæ–‡æœ¬åºåˆ—çš„é•¿åº¦ã€‚è¿™äº›éƒ½æ˜¯é€šè¿‡å¤šå±‚çº§è”çš„Transformerçš„decoderå®ç°çš„ã€‚é€šè¿‡æ¢¯åº¦ä¸‹é™çš„è®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥ä½¿å¾—ä¼¼ç„¶å‡½æ•°æœ€å¤§ã€‚

## æœ‰ç›‘ç£çš„å¾®è°ƒ

GPTä¸­çš„Supervised fine-tuningæ˜¯æŒ‡åœ¨å®Œæˆäº†æ— ç›‘ç£çš„é¢„è®­ç»ƒåï¼Œä½¿ç”¨æœ‰æ ‡æ³¨æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚

<figure><img src="../../.gitbook/assets/language_understanding_paper_21.jpg" alt=""><figcaption></figcaption></figure>

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå·²ç»é¢„è®­ç»ƒå¥½çš„GPTæ¨¡å‹ï¼Œå®ƒçš„å‚æ•°ä¸º $$\theta$$ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æƒ³å°†è¿™ä¸ªæ¨¡å‹åº”ç”¨äºä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç”± $$N$$ä¸ªæ ·æœ¬ç»„æˆçš„è®­ç»ƒé›†ï¼Œå…¶ä¸­ç¬¬ $$i$$ä¸ªæ ·æœ¬çš„è¾“å…¥ä¸º $$x_i$$ï¼Œå¯¹åº”çš„æ ‡ç­¾ä¸º $$y_i$$ã€‚

åœ¨è¿›è¡ŒSupervised fine-tuningæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹GPTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬å¯ä»¥å°†GPTæ¨¡å‹çš„è¾“å‡ºå±‚è¿›è¡Œä¿®æ”¹ï¼Œä¾‹å¦‚æ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œå¹¶å°†å…¶è¿æ¥åˆ°GPTæ¨¡å‹çš„æœ€åä¸€ä¸ªéšè—å±‚ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä¿®æ”¹åçš„æ¨¡å‹è¡¨ç¤ºä¸º $$GPT_{\text{ft}}(\cdot;\theta_{\text{ft}})$$ï¼Œå…¶ä¸­ $$\theta_{\text{ft}}$$æ˜¯å¾®è°ƒåçš„å‚æ•°ã€‚

å¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•° $$L_{\text{cls}}$$ï¼Œclsä»£è¡¨è¾“å…¥çš„å¼€ç«¯ï¼ŒæŸå¤±å‡½æ•°ç”¨äºè¡¡é‡æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œå‡æ–¹è¯¯å·®æŸå¤±ç­‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•° $$L_{\text{cls}}$$ï¼Œä»¥é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥å®ç°Supervised fine-tuningï¼š

1. å°†é¢„è®­ç»ƒå¥½çš„GPTæ¨¡å‹çš„è¾“å‡ºå±‚è¿›è¡Œä¿®æ”¹ï¼Œå¾—åˆ°ä¿®æ”¹åçš„æ¨¡å‹ $$GPT_{\text{ft}}(\cdot;\theta_{\text{ft}})$$ã€‚
2. åœ¨è®­ç»ƒé›†ä¸Šå¯¹ä¿®æ”¹åçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¿™é‡Œå’Œé¢„è®­ç»ƒçš„æ–‡æœ¬é›†åˆä¸åŒï¼ŒFine-Tuningä½¿ç”¨çš„æ˜¯å¸¦æœ‰æ ‡ç­¾çš„æ•°æ®é›†ï¼Œå¦‚æƒ…æ„Ÿåˆ†ç±»ã€æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç­‰ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œé¢„è®­ç»ƒçš„é›†åˆæ˜¯æ— æ ‡ç­¾çš„ã€‚æœ€å°åŒ–æŸå¤±å‡½æ•° $$L_{\text{cls}}$$ã€‚å¯ä»¥ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚
3. å¾®è°ƒå®Œæˆåï¼Œä½¿ç”¨æµ‹è¯•é›†å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è®¡ç®—æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€F1å€¼ç­‰ã€‚

Supervised fine-tuningçš„æ•°å­¦è¡¨ç¤ºå¯ä»¥å¦‚ä¸‹è¡¨ç¤ºï¼š

$$\min_{\theta_{\mathrm{ft}}}\frac{1}{N}\sum_{i=1}^N L_{\mathrm{cls}}(GPT_{\mathrm{ft}}(x_i;\theta_{\mathrm{ft}}),y_i)\quad\quad\text{}$$

å…¶ä¸­ï¼Œ$L\_{\text{cls\}}(\cdot, \cdot)$è¡¨ç¤ºåˆ†ç±»ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œ$x\_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬çš„è¾“å…¥ï¼Œ$y\_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬çš„æ ‡ç­¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å¾®è°ƒåçš„å‚æ•°$\theta\_{\text{ft\}}$ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æŸå¤±å‡½æ•°æœ€å°ã€‚

åœ¨ Improving Language Understanding by Generative Pre-Training è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œç”¨äºåœ¨ GPT ä¸­è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒçš„è¿‡ç¨‹ä¸­åªç”¨åˆ°äº†$12$å±‚çš„decoderç½‘ç»œã€‚

## GPTçš„pytorchå®ç°

é¦–å…ˆï¼Œéœ€è¦å¯¼å…¥éœ€è¦ç”¨åˆ°çš„åº“å’Œæ¨¡å—ï¼š

```python
import torch
import torch.nn as nn
from torch.nn import functional as F

```

æ¥ä¸‹æ¥ï¼Œå®šä¹‰GPTæ¨¡å‹çš„ä¸»è¦ç»„æˆéƒ¨åˆ†â€”â€”Transformer Decoderã€‚è¿™é‡Œæˆ‘ä»¬å‚è€ƒGPT-2ï¼Œä½¿ç”¨12ä¸ªTransformer Decoderæ¥æ„å»ºæ•´ä¸ªæ¨¡å‹ã€‚åœ¨æ¯ä¸ªTransformer Decoderä¸­ï¼Œéƒ½åŒ…å«ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆmulti-head self-attentionï¼‰ï¼Œä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆfeedforward neural networkï¼‰å’Œä¸€ä¸ªæ®‹å·®è¿æ¥ï¼ˆresidual connectionï¼‰ï¼š

```python
class TransformerDecoder(nn.Module):
    def __init__(self, hidden_dim, num_heads, ff_dim, dropout):
        super().__init__()

        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        self.dropout1 = nn.Dropout(dropout)
        self.layer_norm1 = nn.LayerNorm(hidden_dim)

        self.ff = nn.Sequential(
            nn.Linear(hidden_dim, ff_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, hidden_dim),
        )
        self.dropout2 = nn.Dropout(dropout)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)

    def forward(self, x, mask):
        # Multi-head self-attention
        attn_out, _ = self.multihead_attn(x, x, x, attn_mask=mask)
        attn_out = self.dropout1(attn_out)
        x = self.layer_norm1(x + attn_out)

        # Feedforward neural network
        ff_out = self.ff(x)
        ff_out = self.dropout2(ff_out)
        x = self.layer_norm2(x + ff_out)

        return x

```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›Transformer Decoderä¸²è”èµ·æ¥ï¼Œå½¢æˆæ•´ä¸ªGPTæ¨¡å‹ï¼š

```python
class GPT(nn.Module):
    def __init__(self, num_tokens, hidden_dim, num_heads, num_layers, seq_len, dropout):
        super().__init__()

        self.token_emb = nn.Embedding(num_tokens, hidden_dim)
        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, hidden_dim))
        self.dropout = nn.Dropout(dropout)

        self.decoders = nn.ModuleList([
            TransformerDecoder(hidden_dim, num_heads, hidden_dim * 4, dropout)
            for _ in range(num_layers)
        ])

        self.output_layer = nn.Linear(hidden_dim, num_tokens)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        # Token embeddings
        x = self.token_emb(x)

        # Add position embeddings
        x += self.pos_emb[:, :x.shape[1]]

        # Transformer Decoder layers
        mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool().to(x.device)
        for decoder in self.decoders:
            x = decoder(x, mask)

        # Output layer
        x = self.output_layer(x)
        x = self.softmax(x)

        return x

```

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ç­‰ï¼š

```python
model = GPT(num_tokens, hidden_dim, num_heads, num_layers, seq_len, dropout)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

for epoch in range(num_epochs):
    for inputs, labels in dat

```
